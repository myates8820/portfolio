{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will begin to start implementing the ML algorithms. We will be eventually creating a pipeline for bagging and\n",
    "# ensembling several estimators in order to create the most robust algorith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, start out by importing the libraries we expect to use\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pipeline, transformations, and CV/scoring\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_validate, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up steps to import and split our data for the algorithm. However, we are going to scale our data in order to be more\n",
    "# effective with the various ML algorithms\n",
    "# next time set index to false for to_csv\n",
    "\n",
    "df = pd.read_csv('ml_ready.csv').drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "df['YearBuiltSq'] = df['YearBuilt']*df['YearBuilt']\n",
    "\n",
    "# df = pd.read_csv('ml_ready.csv').drop(['Unnamed: 0', 'Unnamed: 0.1', 'GrLivAreaNegSq', 'TotalBsmtSFNegSq'], axis=1)\n",
    "# old df, with corrected models we can now fit the extra patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a quick transformer to unlog our predictions\n",
    "def transformer(num):\n",
    "    return np.exp(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next time in original data exploration/transformation functions, incorporate dtype switch\n",
    "# creating complete list of categorical variables, afterwards will re-merge df\n",
    "categorical = []\n",
    "for x in list(df.columns):\n",
    "    if 'Non' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'Has' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'SubClass' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'SaleConditionNor' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'LandSlopeGtl' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'LotShapeReg' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'HeatingGas' in x:\n",
    "        categorical.append(x)      \n",
    "    elif 'CentralAir' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'MoSold' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'YrSold' in x:\n",
    "        categorical.append(x)\n",
    "    \n",
    "for x in list(df.select_dtypes(include='object').columns):\n",
    "    categorical.append(x)\n",
    "categorical = list(set(categorical))\n",
    "\n",
    "# merging converted dfs\n",
    "\n",
    "df[categorical] = df[categorical].astype('object')\n",
    "df_num = df.drop(categorical, axis = 1)\n",
    "df = pd.concat([df[categorical],df_num], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the X and y variables\n",
    "\n",
    "X = df.drop(['LogSale','SalePrice'],axis=1)\n",
    "X = pd.get_dummies(X,drop_first=True)\n",
    "y = df['LogSale']\n",
    "\n",
    "# creating test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing regression models\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, Lasso, Ridge, LinearRegression, ElasticNetCV, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to ignore warnings to ignore the deprecation warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_test_score  std_test_score  mean_train_score  std_train_score\n",
      "0         0.895655        0.007777          0.905028          0.00601\n",
      "1         0.895655        0.007777          0.905028          0.00601\n",
      "2         0.895655        0.007777          0.905028          0.00601\n",
      "0   -0.009373\n",
      "1   -0.009373\n",
      "2   -0.009373\n",
      "dtype: float64\n",
      "0.1261680685815775\n",
      "0.8827279650298142\n"
     ]
    }
   ],
   "source": [
    "pipe_lasso = [('scaler', RobustScaler()),\n",
    "             ('clf', LassoCV())]\n",
    "clf_lasso = Pipeline(pipe_lasso)\n",
    "params = {'clf__n_alphas': [1,100,500]}\n",
    "grid = GridSearchCV(clf_lasso, param_grid = params, cv = 5)\n",
    "best_lasso = grid.fit(X_train,y_train).best_params_\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(results[['mean_test_score','std_test_score','mean_train_score','std_train_score']])\n",
    "print(results['mean_test_score']-results['mean_train_score'])\n",
    "\n",
    "clf_lasso.fit(X_train,y_train)\n",
    "lasso_preds = clf_lasso.predict(X_test)\n",
    "print(np.sqrt(mean_squared_log_error(transformer(y_test),transformer(lasso_preds))))\n",
    "print(r2_score(transformer(y_test),transformer(lasso_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02484082,  0.01273519, -0.00208323])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lasso = [('scaler', RobustScaler()),\n",
    "             ('clf', LassoCV(n_alphas = best_lasso['clf__n_alphas']))]\n",
    "clf_lasso = Pipeline(pipe_lasso)\n",
    "scores = cross_validate(clf_lasso, X_train, y_train, scoring = 'r2', return_train_score=True)\n",
    "scores['train_score']-scores['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_test_score  std_test_score  mean_train_score  std_train_score\n",
      "0         0.895125        0.007497          0.904397         0.006194\n",
      "1         0.895125        0.007497          0.904397         0.006194\n",
      "2         0.895125        0.007497          0.904397         0.006194\n",
      "0   -0.009272\n",
      "1   -0.009272\n",
      "2   -0.009272\n",
      "dtype: float64\n",
      "0.12593926570931568\n",
      "0.8830801435048478\n"
     ]
    }
   ],
   "source": [
    "pipe_elastic = [('scaler', RobustScaler()),\n",
    "             ('clf', ElasticNetCV())]\n",
    "clf_elastic = Pipeline(pipe_elastic)\n",
    "params = {'clf__n_alphas': [1,100,500]}\n",
    "grid = GridSearchCV(clf_elastic, param_grid = params, cv = 5)\n",
    "best_elastic = grid.fit(X_train,y_train).best_params_\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(results[['mean_test_score','std_test_score','mean_train_score','std_train_score']])\n",
    "print(results['mean_test_score']-results['mean_train_score'])\n",
    "\n",
    "clf_elastic.fit(X_train,y_train)\n",
    "elastic_preds = clf_elastic.predict(X_test)\n",
    "print(np.sqrt(mean_squared_log_error(transformer(y_test),transformer(elastic_preds))))\n",
    "print(r2_score(transformer(y_test),transformer(elastic_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02539887,  0.012705  , -0.00317776])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_elastic = [('scaler', RobustScaler()),\n",
    "             ('clf', ElasticNetCV(n_alphas = best_elastic['clf__n_alphas']))]\n",
    "clf_elastic = Pipeline(pipe_elastic)\n",
    "scores = cross_validate(clf_elastic, X_train, y_train, scoring = 'r2', return_train_score=True)\n",
    "scores['train_score']-scores['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_test_score  std_test_score  mean_train_score  std_train_score\n",
      "0         0.913286        0.002216          0.944489         0.000418\n",
      "1         0.913286        0.002216          0.944489         0.000418\n",
      "0   -0.031203\n",
      "1   -0.031203\n",
      "dtype: float64\n",
      "0.11664547100297276\n",
      "0.9095162264241541\n"
     ]
    }
   ],
   "source": [
    "pipe_ridge = [('scaler', RobustScaler()),\n",
    "             ('clf', RidgeCV())]\n",
    "clf_ridge = Pipeline(pipe_ridge)\n",
    "params = {'clf__cv': [3,5]}\n",
    "grid = GridSearchCV(clf_ridge, param_grid = params, cv = 5)\n",
    "best_ridge = grid.fit(X_train,y_train).best_params_\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(results[['mean_test_score','std_test_score','mean_train_score','std_train_score']])\n",
    "print(results['mean_test_score']-results['mean_train_score'])\n",
    "\n",
    "clf_ridge.fit(X_train,y_train)\n",
    "ridge_preds = clf_ridge.predict(X_test)\n",
    "print(np.sqrt(mean_squared_log_error(transformer(y_test),transformer(ridge_preds))))\n",
    "print(r2_score(transformer(y_test),transformer(ridge_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ridge['clf__cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04234309, 0.04079464, 0.02094412])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_ridge = [('scaler', RobustScaler()),\n",
    "             ('clf', RidgeCV(cv = best_ridge['clf__cv']))]\n",
    "clf_ridge = Pipeline(pipe_ridge)\n",
    "scores = cross_validate(clf_ridge, X_train, y_train, scoring = 'r2', return_train_score=True)\n",
    "scores['train_score']-scores['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our transformer to deal with numerical data, have already dummied categorical variables so we don't need to address\n",
    "# those variables. However, will address skewed numerical variables before putting through to the standard scaler for all \n",
    "# numerical variables.\n",
    "\n",
    "# list of highly skewed numerical variables\n",
    "numerical = list(X.select_dtypes(include = ['int64', 'float64']).columns)\n",
    "skewed = X[numerical].skew(axis=0).reset_index()\n",
    "skewed_vars = list(skewed[abs(skewed[0])>2]['index'])\n",
    "\n",
    "\n",
    "# making the transformers\n",
    "numerical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy = 'median')),\n",
    "                                ('scaler', RobustScaler())])\n",
    "skewed_transformer = Pipeline(steps = [('skew_scaler', PowerTransformer())])\n",
    "\n",
    "\n",
    "# preprocessing function to be implemented into the pipeline\n",
    "preprocess = ColumnTransformer(transformers=[('num', numerical_transformer, numerical)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_test_score  std_test_score  mean_train_score  std_train_score\n",
      "0          0.834681        0.027688          0.880553         0.002855\n",
      "1          0.839129        0.020054          0.887750         0.002050\n",
      "2          0.844494        0.024654          0.889248         0.004993\n",
      "3          0.842844        0.020902          0.892781         0.002888\n",
      "4          0.839532        0.017240          0.898908         0.002858\n",
      "5          0.850406        0.021069          0.902509         0.002493\n",
      "6          0.847872        0.022002          0.903336         0.003761\n",
      "7          0.851918        0.020972          0.904531         0.001219\n",
      "8          0.848599        0.020146          0.905151         0.002587\n",
      "9          0.850507        0.014774          0.907382         0.002979\n",
      "10         0.856578        0.016326          0.909372         0.002024\n",
      "11         0.855356        0.020484          0.910799         0.002987\n",
      "12         0.852089        0.015250          0.961767         0.002431\n",
      "13         0.863505        0.016838          0.966786         0.001768\n",
      "14         0.871693        0.016585          0.969392         0.001241\n",
      "15         0.874671        0.019068          0.969933         0.001690\n",
      "16         0.862909        0.010629          0.966603         0.001695\n",
      "17         0.874630        0.015452          0.970087         0.001917\n",
      "18         0.871125        0.010680          0.971956         0.001095\n",
      "19         0.878209        0.012416          0.972729         0.000921\n",
      "20         0.862428        0.013851          0.967795         0.001248\n",
      "21         0.875072        0.015021          0.971164         0.001226\n",
      "22         0.876559        0.014192          0.973212         0.000889\n",
      "23         0.873865        0.010556          0.975122         0.001704\n",
      "24         0.852363        0.021548          0.972760         0.001419\n",
      "25         0.861896        0.015770          0.975588         0.001094\n",
      "26         0.871072        0.017160          0.977831         0.000621\n",
      "27         0.871483        0.019193          0.978294         0.001014\n",
      "28         0.864369        0.011083          0.973829         0.001255\n",
      "29         0.867983        0.015903          0.976460         0.000790\n",
      "30         0.872551        0.014061          0.978760         0.001312\n",
      "31         0.871612        0.015245          0.980685         0.001298\n",
      "32         0.861368        0.014893          0.975269         0.001320\n",
      "33         0.870358        0.017531          0.977235         0.001204\n",
      "34         0.876435        0.011983          0.978730         0.001616\n",
      "35         0.877090        0.012970          0.980447         0.000750\n",
      "36         0.865725        0.013611          0.971991         0.001633\n",
      "37         0.870624        0.023227          0.976474         0.001303\n",
      "38         0.871465        0.012184          0.977683         0.001547\n",
      "39         0.872713        0.017750          0.978926         0.000529\n",
      "40         0.864800        0.010611          0.974144         0.001832\n",
      "41         0.871405        0.012621          0.976688         0.000790\n",
      "42         0.871154        0.016821          0.979313         0.000410\n",
      "43         0.874270        0.014014          0.980226         0.000804\n",
      "44         0.862081        0.015659          0.973113         0.001423\n",
      "45         0.871240        0.015527          0.976746         0.000940\n",
      "46         0.870535        0.016433          0.979293         0.001637\n",
      "47         0.875509        0.010363          0.979946         0.001490\n",
      "48         0.864419        0.009312          0.973883         0.001636\n",
      "49         0.871745        0.014541          0.974983         0.002032\n",
      "50         0.874042        0.018562          0.977286         0.001510\n",
      "51         0.871366        0.015789          0.978874         0.000947\n",
      "52         0.861674        0.015765          0.974171         0.001695\n",
      "53         0.868679        0.006718          0.978387         0.001291\n",
      "54         0.872870        0.015772          0.978496         0.001200\n",
      "55         0.874108        0.015660          0.980238         0.001375\n",
      "56         0.857216        0.011506          0.974710         0.001237\n",
      "57         0.869859        0.012785          0.977626         0.000921\n",
      "58         0.874431        0.014467          0.977962         0.001005\n",
      "59         0.874569        0.014088          0.979634         0.000689\n",
      "0    -0.045871\n",
      "1    -0.048621\n",
      "2    -0.044754\n",
      "3    -0.049937\n",
      "4    -0.059375\n",
      "5    -0.052103\n",
      "6    -0.055463\n",
      "7    -0.052614\n",
      "8    -0.056552\n",
      "9    -0.056875\n",
      "10   -0.052793\n",
      "11   -0.055442\n",
      "12   -0.109677\n",
      "13   -0.103281\n",
      "14   -0.097700\n",
      "15   -0.095262\n",
      "16   -0.103694\n",
      "17   -0.095457\n",
      "18   -0.100831\n",
      "19   -0.094520\n",
      "20   -0.105367\n",
      "21   -0.096092\n",
      "22   -0.096653\n",
      "23   -0.101257\n",
      "24   -0.120397\n",
      "25   -0.113692\n",
      "26   -0.106759\n",
      "27   -0.106811\n",
      "28   -0.109460\n",
      "29   -0.108477\n",
      "30   -0.106209\n",
      "31   -0.109074\n",
      "32   -0.113900\n",
      "33   -0.106877\n",
      "34   -0.102296\n",
      "35   -0.103357\n",
      "36   -0.106266\n",
      "37   -0.105850\n",
      "38   -0.106218\n",
      "39   -0.106213\n",
      "40   -0.109343\n",
      "41   -0.105283\n",
      "42   -0.108159\n",
      "43   -0.105956\n",
      "44   -0.111032\n",
      "45   -0.105505\n",
      "46   -0.108759\n",
      "47   -0.104437\n",
      "48   -0.109464\n",
      "49   -0.103238\n",
      "50   -0.103244\n",
      "51   -0.107508\n",
      "52   -0.112498\n",
      "53   -0.109708\n",
      "54   -0.105625\n",
      "55   -0.106130\n",
      "56   -0.117494\n",
      "57   -0.107767\n",
      "58   -0.103531\n",
      "59   -0.105065\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pipe_rf = [('preprocess', preprocess),\n",
    "             ('clf', RandomForestRegressor())]\n",
    "clf_rf = Pipeline(pipe_rf)\n",
    "params = {'clf__max_depth': range(5,26,5),\n",
    "         'clf__max_features': range(5,20,5),\n",
    "          'clf__n_estimators': [10,15,20,25]}\n",
    "grid = GridSearchCV(clf_rf, param_grid = params, cv = 5)\n",
    "best_rf = grid.fit(X_train,y_train).best_params_\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(results[['mean_test_score','std_test_score','mean_train_score','std_train_score']])\n",
    "print(results['mean_test_score']-results['mean_train_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 10, 'clf__max_features': 10, 'clf__n_estimators': 25}\n"
     ]
    }
   ],
   "source": [
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Scores\n",
      "[0.97615269 0.97448584 0.97333396]\n",
      "\n",
      "Test Scores\n",
      "[0.85070971 0.87578994 0.88962127]\n",
      "\n",
      "Train - Test Scores\n",
      "\n",
      "Appears to be some overfitting with the random forest regressor, however, we will still try to use in our stacked model\n"
     ]
    }
   ],
   "source": [
    "pipe_rf = [('scaler', preprocess),\n",
    "             ('clf', RandomForestRegressor(max_depth = best_rf['clf__max_depth'],\n",
    "                                           max_features = best_rf['clf__max_features'],\n",
    "                                           n_estimators = best_rf['clf__n_estimators']))]\n",
    "clf_elastic = Pipeline(pipe_rf)\n",
    "scores = cross_validate(clf_elastic, X_train, y_train, scoring = 'r2', return_train_score=True)\n",
    "print('Train Scores')\n",
    "print(scores['train_score'])\n",
    "print()\n",
    "print('Test Scores')\n",
    "print(scores['test_score'])\n",
    "print()\n",
    "print('Train - Test Scores')\n",
    "scores['train_score']-scores['test_score']\n",
    "\n",
    "print()\n",
    "print('Appears to be some overfitting with the random forest regressor, however, we will still try to use in our stacked model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall, appears our models are doing well with not too high of variance and low bias. Going to create a stacked model with\n",
    "# all four models\n",
    "\n",
    "#going to reshuffle the train-test splot\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([ 0.1,  1. , 10. ]), cv=5, fit_intercept=True,\n",
       "    gcv_mode=None, normalize=False, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up our first level predictions\n",
    "clf_lasso = LassoCV(cv=5)\n",
    "clf_elastic = ElasticNetCV(cv=5)\n",
    "clf_ridge = RidgeCV(cv=5)\n",
    "\n",
    "classifiers = [clf_lasso, clf_elastic, clf_ridge]\n",
    "predictions = []\n",
    "pred_cols = []\n",
    "\n",
    "for x in classifiers:\n",
    "    pipe_steps = [('preprocess', preprocess),\n",
    "                 ('clf', x)]\n",
    "    pipe = Pipeline(pipe_steps)\n",
    "    pred_name = 'pred_' + str(x)[0: str(x).find('(')]\n",
    "    pred_cols.append(pred_name)\n",
    "    pipe.fit(X_train,y_train)\n",
    "    predictions.append(transformer(pipe.predict(X_train)))\n",
    "    \n",
    "first_level_preds = pd.DataFrame({'pred_lasso': predictions[0],\n",
    "                                 'pred_elastic': predictions[1],\n",
    "                                 'pred_ridge': predictions[2]})\n",
    "\n",
    "X_train_2nd = pd.concat([X_train.reset_index(),first_level_preds], axis=1).set_index(keys='index')\n",
    "\n",
    "stacked = RidgeCV(cv = 5)\n",
    "stacked.fit(X_train_2nd,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91832222 0.92397493 0.93761154 0.88947616 0.91107905]\n",
      "0.11670295122925003\n",
      "0.9218382561389564\n"
     ]
    }
   ],
   "source": [
    "# setting up our second level predictions\n",
    "\n",
    "predictions = []\n",
    "pred_cols = []\n",
    "\n",
    "for x in classifiers:\n",
    "    pipe_steps = [('preprocess', preprocess),\n",
    "                 ('clf', x)]\n",
    "    pipe = Pipeline(pipe_steps)\n",
    "    pred_name = 'pred_' + str(x)[0: str(x).find('(')]\n",
    "    pred_cols.append(pred_name)\n",
    "    predictions.append(transformer(pipe.predict(X_test)))\n",
    "    \n",
    "test_level_preds = pd.DataFrame({'pred_lasso': predictions[0],\n",
    "                                 'pred_elastic': predictions[1],\n",
    "                                 'pred_ridge': predictions[2]})\n",
    "\n",
    "X_test_1st = pd.concat([X_test.reset_index(),test_level_preds], axis=1).set_index(keys='index')\n",
    "\n",
    "results = cross_val_score(stacked, X_train, y_train, scoring = 'r2', cv = 5)\n",
    "print(results)\n",
    "\n",
    "stacked_preds = stacked.predict(X_test_1st)\n",
    "print(np.sqrt(mean_squared_log_error(transformer(y_test),transformer(stacked_preds))))\n",
    "print(r2_score(transformer(y_test),transformer(stacked_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There we go. As we can see, we were able to improve our model slightly by stacking our regressors. In the future, we could\n",
    "# try to add some more regressors with high performance that were slightly less correlated with our other models to try improve\n",
    "# the stacking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for if I decide to try with dropped variables, although no longer necessary \\n\\n\\ndrop_list = []\\ndrop_list.append('Condition1')\\ndrop_list.append('HasGarage')\\ndrop_list.append('LotConfig')\\ndrop_list.append('RoofStyle')\\ndrop_list.append('BsmtFinType1')\\ndrop_list.append('Functional')\\ndrop_list.append('LandContour')\\ndrop_list.append('GarageType')\\n\\nfor x in list(df.select_dtypes(include = ['int64', 'float64']).columns):\\n    if '*' in x:\\n        drop_list.append(x)\\n        \\ndf = pd.read_csv('ml_ready.csv').drop(['Unnamed: 0', 'Unnamed: 0.1', 'GrLivAreaNegSq', 'TotalBsmtSFNegSq'], axis=1)\\ndf = df.drop(drop_list, axis=1)\\ndf['YearBuiltSq'] = df['YearBuilt']*df['YearBuilt']\\n\\ncategorical = []\\nfor x in list(df.columns):\\n    if 'Non' in x:\\n        categorical.append(x)\\n    elif 'Has' in x:\\n        categorical.append(x)\\n    elif 'SubClass' in x:\\n        categorical.append(x)\\n    elif 'SaleConditionNor' in x:\\n        categorical.append(x)\\n    elif 'LandSlopeGtl' in x:\\n        categorical.append(x)\\n    elif 'LotShapeReg' in x:\\n        categorical.append(x)\\n    elif 'HeatingGas' in x:\\n        categorical.append(x)      \\n    elif 'CentralAir' in x:\\n        categorical.append(x)\\n    elif 'MoSold' in x:\\n        categorical.append(x)\\n    elif 'YrSold' in x:\\n        categorical.append(x)\\n    \\nfor x in list(df.select_dtypes(include='object').columns):\\n    categorical.append(x)\\ncategorical = list(set(categorical))\\n\\n# merging converted dfs\\n\\ndf[categorical] = df[categorical].astype('object')\\ndf_num = df.drop(categorical, axis = 1)\\ndf = pd.concat([df[categorical],df_num], axis=1)\\n\\n# creating the X and y variables\\n\\nX = df.drop(['LogSale','SalePrice'],axis=1)\\nX = pd.get_dummies(X,drop_first=True)\\ny = df['LogSale']\\n\\nnumerical = list(X.select_dtypes(include = ['int64', 'float64']).columns)\\nskewed = X[numerical].skew(axis=0).reset_index()\\nskewed_vars = list(skewed[abs(skewed[0])>2]['index'])\\n\\n# creating test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33)\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" for if I decide to try with dropped variables, although no longer necessary \n",
    "\n",
    "\n",
    "drop_list = []\n",
    "drop_list.append('Condition1')\n",
    "drop_list.append('HasGarage')\n",
    "drop_list.append('LotConfig')\n",
    "drop_list.append('RoofStyle')\n",
    "drop_list.append('BsmtFinType1')\n",
    "drop_list.append('Functional')\n",
    "drop_list.append('LandContour')\n",
    "drop_list.append('GarageType')\n",
    "\n",
    "for x in list(df.select_dtypes(include = ['int64', 'float64']).columns):\n",
    "    if '*' in x:\n",
    "        drop_list.append(x)\n",
    "        \n",
    "df = pd.read_csv('ml_ready.csv').drop(['Unnamed: 0', 'Unnamed: 0.1', 'GrLivAreaNegSq', 'TotalBsmtSFNegSq'], axis=1)\n",
    "df = df.drop(drop_list, axis=1)\n",
    "df['YearBuiltSq'] = df['YearBuilt']*df['YearBuilt']\n",
    "\n",
    "categorical = []\n",
    "for x in list(df.columns):\n",
    "    if 'Non' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'Has' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'SubClass' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'SaleConditionNor' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'LandSlopeGtl' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'LotShapeReg' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'HeatingGas' in x:\n",
    "        categorical.append(x)      \n",
    "    elif 'CentralAir' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'MoSold' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'YrSold' in x:\n",
    "        categorical.append(x)\n",
    "    \n",
    "for x in list(df.select_dtypes(include='object').columns):\n",
    "    categorical.append(x)\n",
    "categorical = list(set(categorical))\n",
    "\n",
    "# merging converted dfs\n",
    "\n",
    "df[categorical] = df[categorical].astype('object')\n",
    "df_num = df.drop(categorical, axis = 1)\n",
    "df = pd.concat([df[categorical],df_num], axis=1)\n",
    "\n",
    "# creating the X and y variables\n",
    "\n",
    "X = df.drop(['LogSale','SalePrice'],axis=1)\n",
    "X = pd.get_dummies(X,drop_first=True)\n",
    "y = df['LogSale']\n",
    "\n",
    "numerical = list(X.select_dtypes(include = ['int64', 'float64']).columns)\n",
    "skewed = X[numerical].skew(axis=0).reset_index()\n",
    "skewed_vars = list(skewed[abs(skewed[0])>2]['index'])\n",
    "\n",
    "# creating test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow_Enabled",
   "language": "python",
   "name": "tensorflow_enabled"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
