{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will begin to start implementing the ML algorithms. We will be eventually creating a pipeline for bagging and\n",
    "# ensembling several estimators in order to create the most robust algorith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, start out by importing the libraries we expect to use\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pipeline, transformations, and CV/scoring\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_validate, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up steps to import and split our data for the algorithm. However, we are going to scale our data in order to be more\n",
    "# effective with the various ML algorithms\n",
    "# next time set index to false for to_csv\n",
    "\n",
    "df = pd.read_csv('ml_ready.csv').drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "df['YearBuiltSq'] = df['YearBuilt']*df['YearBuilt']\n",
    "\n",
    "# df = pd.read_csv('ml_ready.csv').drop(['Unnamed: 0', 'Unnamed: 0.1', 'GrLivAreaNegSq', 'TotalBsmtSFNegSq'], axis=1)\n",
    "# old df, with corrected models we can now fit the extra patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a quick transformer to unlog our predictions\n",
    "def transformer(num):\n",
    "    return np.exp(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next time in original data exploration/transformation functions, incorporate dtype switch\n",
    "# creating complete list of categorical variables, afterwards will re-merge df\n",
    "categorical = []\n",
    "for x in list(df.columns):\n",
    "    if 'Non' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'Has' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'SubClass' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'SaleConditionNor' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'LandSlopeGtl' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'LotShapeReg' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'HeatingGas' in x:\n",
    "        categorical.append(x)      \n",
    "    elif 'CentralAir' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'MoSold' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'YrSold' in x:\n",
    "        categorical.append(x)\n",
    "    \n",
    "for x in list(df.select_dtypes(include='object').columns):\n",
    "    categorical.append(x)\n",
    "categorical = list(set(categorical))\n",
    "\n",
    "# merging converted dfs\n",
    "\n",
    "df[categorical] = df[categorical].astype('object')\n",
    "df_num = df.drop(categorical, axis = 1)\n",
    "df = pd.concat([df[categorical],df_num], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the X and y variables\n",
    "\n",
    "X = df.drop(['LogSale','SalePrice'],axis=1)\n",
    "X = pd.get_dummies(X,drop_first=True)\n",
    "y = df['LogSale']\n",
    "\n",
    "# creating test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing regression models\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, Lasso, Ridge, LinearRegression, ElasticNetCV, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to ignore warnings to ignore the deprecation warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__alpha': 0.001}\n",
      "   mean_test_score  std_test_score  mean_train_score  std_train_score\n",
      "0         0.896585        0.007982          0.948758         0.001428\n",
      "1         0.896350        0.006087          0.917371         0.003174\n",
      "2         0.707029        0.023850          0.712811         0.011790\n",
      "3        -0.011356        0.008091          0.000000         0.000000\n",
      "4        -0.011356        0.008091          0.000000         0.000000\n",
      "0   -0.052173\n",
      "1   -0.021021\n",
      "2   -0.005782\n",
      "3   -0.011356\n",
      "4   -0.011356\n",
      "dtype: float64\n",
      "0.11419909330804812\n",
      "0.9330587801563293\n"
     ]
    }
   ],
   "source": [
    "pipe_lasso = [('scaler', StandardScaler()),\n",
    "             ('clf', Lasso())]\n",
    "clf_lasso = Pipeline(pipe_lasso)\n",
    "params = {'clf__alpha': [0.001,0.01,0.1,1,10]}\n",
    "grid = GridSearchCV(clf_lasso, param_grid = params, cv = 3,return_train_score=True)\n",
    "best_lasso = grid.fit(X_train,y_train).best_params_\n",
    "print(best_lasso)\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(results[['mean_test_score','std_test_score','mean_train_score','std_train_score']])\n",
    "print(results['mean_test_score']-results['mean_train_score'])\n",
    "\n",
    "pipe_lasso = [('scaler', RobustScaler()),\n",
    "             ('clf', Lasso(alpha = best_lasso['clf__alpha']))]\n",
    "clf_lasso = Pipeline(pipe_lasso)\n",
    "clf_lasso.fit(X_train,y_train)\n",
    "lasso_preds = clf_lasso.predict(X_test)\n",
    "print(np.sqrt(mean_squared_log_error(transformer(y_test),transformer(lasso_preds))))\n",
    "print(r2_score(transformer(y_test),transformer(lasso_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0472431  0.01810335 0.01352342]\n",
      "0.014933621567095809\n"
     ]
    }
   ],
   "source": [
    "pipe_lasso = [('scaler', RobustScaler()),\n",
    "             ('clf', Lasso(alpha = best_lasso['clf__alpha']))]\n",
    "clf_lasso = Pipeline(pipe_lasso)\n",
    "scores = cross_validate(clf_lasso, X_train, y_train, scoring = 'r2', return_train_score=True)\n",
    "print(scores['train_score']-scores['test_score'])\n",
    "print(np.std(scores['train_score']-scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__eps': 0.0001}\n",
      "\n",
      "   mean_test_score  std_test_score  mean_train_score  std_train_score\n",
      "0         0.903735        0.007802          0.938748         0.002718\n",
      "1         0.903662        0.007755          0.938703         0.003153\n",
      "2         0.903662        0.007755          0.938703         0.003153\n",
      "0   -0.035013\n",
      "1   -0.035041\n",
      "2   -0.035041\n",
      "dtype: float64\n",
      "0.11466676747456364\n",
      "0.9315844957002128\n"
     ]
    }
   ],
   "source": [
    "pipe_lasso = [('scaler', StandardScaler()),\n",
    "             ('clf', LassoCV())]\n",
    "clf_lasso = Pipeline(pipe_lasso)\n",
    "params = {\n",
    "         'clf__eps': [0.0001, 0.001, 0.01]}\n",
    "grid = GridSearchCV(clf_lasso, param_grid = params, cv = 3,return_train_score = True)\n",
    "best_lasso = grid.fit(X_train,y_train).best_params_\n",
    "\n",
    "print(best_lasso)\n",
    "print()\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(results[['mean_test_score','std_test_score','mean_train_score','std_train_score']])\n",
    "print(results['mean_test_score']-results['mean_train_score'])\n",
    "\n",
    "clf_lasso.fit(X_train,y_train)\n",
    "lasso_preds = clf_lasso.predict(X_test)\n",
    "print(np.sqrt(mean_squared_log_error(transformer(y_test),transformer(lasso_preds))))\n",
    "print(r2_score(transformer(y_test),transformer(lasso_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_test_score  std_test_score  mean_train_score  std_train_score\n",
      "0         0.895402         0.00681          0.909704         0.001928\n",
      "1         0.895402         0.00681          0.909704         0.001928\n",
      "2         0.895402         0.00681          0.909704         0.001928\n",
      "0   -0.014302\n",
      "1   -0.014302\n",
      "2   -0.014302\n",
      "dtype: float64\n",
      "0.1217141476956355\n",
      "0.9221578102806048\n"
     ]
    }
   ],
   "source": [
    "pipe_elastic = [('scaler', RobustScaler()),\n",
    "             ('clf', ElasticNetCV())]\n",
    "clf_elastic = Pipeline(pipe_elastic)\n",
    "params = {'clf__n_alphas': [1,100,500]}\n",
    "grid = GridSearchCV(clf_elastic, param_grid = params, cv = 5, return_train_score=True)\n",
    "best_elastic = grid.fit(X_train,y_train).best_params_\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(results[['mean_test_score','std_test_score','mean_train_score','std_train_score']])\n",
    "print(results['mean_test_score']-results['mean_train_score'])\n",
    "\n",
    "clf_elastic.fit(X_train,y_train)\n",
    "elastic_preds = clf_elastic.predict(X_test)\n",
    "print(np.sqrt(mean_squared_log_error(transformer(y_test),transformer(elastic_preds))))\n",
    "print(r2_score(transformer(y_test),transformer(elastic_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03215956 0.01620559 0.00400211]\n",
      "0.011529169796845113\n"
     ]
    }
   ],
   "source": [
    "pipe_elastic = [('scaler', RobustScaler()),\n",
    "             ('clf', ElasticNetCV(n_alphas = best_elastic['clf__n_alphas']))]\n",
    "clf_elastic = Pipeline(pipe_elastic)\n",
    "scores = cross_validate(clf_elastic, X_train, y_train, scoring = 'r2', return_train_score=True)\n",
    "print(scores['train_score']-scores['test_score'])\n",
    "print(np.std(scores['train_score']-scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_test_score  std_test_score  mean_train_score  std_train_score\n",
      "0         0.909231        0.006662          0.940464         0.001335\n",
      "1         0.909231        0.006662          0.940464         0.001335\n",
      "0   -0.031232\n",
      "1   -0.031232\n",
      "dtype: float64\n",
      "0.11650900606033948\n",
      "0.930053558349081\n"
     ]
    }
   ],
   "source": [
    "pipe_ridge = [('scaler', RobustScaler()),\n",
    "             ('clf', RidgeCV())]\n",
    "clf_ridge = Pipeline(pipe_ridge)\n",
    "params = {'clf__cv': [3,5]}\n",
    "grid = GridSearchCV(clf_ridge, param_grid = params, cv = 5, return_train_score=True)\n",
    "best_ridge = grid.fit(X_train,y_train).best_params_\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(results[['mean_test_score','std_test_score','mean_train_score','std_train_score']])\n",
    "print(results['mean_test_score']-results['mean_train_score'])\n",
    "\n",
    "clf_ridge.fit(X_train,y_train)\n",
    "ridge_preds = clf_ridge.predict(X_test)\n",
    "print(np.sqrt(mean_squared_log_error(transformer(y_test),transformer(ridge_preds))))\n",
    "print(r2_score(transformer(y_test),transformer(ridge_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ridge['clf__cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05626941 0.03339184 0.02572749]\n",
      "0.01297405507175217\n"
     ]
    }
   ],
   "source": [
    "pipe_ridge = [('scaler', RobustScaler()),\n",
    "             ('clf', RidgeCV(cv = best_ridge['clf__cv']))]\n",
    "clf_ridge = Pipeline(pipe_ridge)\n",
    "scores = cross_validate(clf_ridge, X_train, y_train, scoring = 'r2', return_train_score=True)\n",
    "print(scores['train_score']-scores['test_score'])\n",
    "print(np.std(scores['train_score']-scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our transformer to deal with numerical data, have already dummied categorical variables so we don't need to address\n",
    "# those variables. In some cases, we would want to deal with the skewed variables by using the PowerTransformer() function from\n",
    "# sci-kit learn, which uses the Yeo-Johnson method for normalizing skewed datasets. However, for the sake of using a single\n",
    "# scaler/transformer, I use the robust scaler which helps with reducing the impact of outliers (which is particularly beneficial\n",
    "# for the use of the Lasso and ElasticNet algorithms), as well as reducing the skew of the variables.\n",
    "\n",
    "# list of highly skewed numerical variables\n",
    "numerical = list(X.select_dtypes(include = ['int64', 'float64']).columns)\n",
    "# skewed = X[numerical].skew(axis=0).reset_index()\n",
    "# skewed_vars = list(skewed[abs(skewed[0])>2]['index'])\n",
    "\n",
    "\n",
    "# making the transformers\n",
    "numerical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy = 'median')),\n",
    "                                ('scaler', RobustScaler())])\n",
    "# skewed_transformer = Pipeline(steps = [('skew_scaler', PowerTransformer())])\n",
    "\n",
    "\n",
    "# preprocessing function to be implemented into the pipeline\n",
    "preprocess = ColumnTransformer(transformers=[('num', numerical_transformer, numerical)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_test_score  std_test_score  mean_train_score  std_train_score\n",
      "0          0.828215        0.017562          0.875399         0.007364\n",
      "1          0.817103        0.009221          0.880399         0.004258\n",
      "2          0.827053        0.011358          0.885590         0.003371\n",
      "3          0.829639        0.011203          0.884693         0.004877\n",
      "4          0.834234        0.021144          0.895071         0.002705\n",
      "5          0.833713        0.012300          0.898321         0.004308\n",
      "6          0.839345        0.013686          0.897966         0.003444\n",
      "7          0.836889        0.012665          0.901731         0.002889\n",
      "8          0.835346        0.015318          0.903685         0.003744\n",
      "9          0.840393        0.012621          0.906239         0.005376\n",
      "10         0.843796        0.011954          0.906259         0.007173\n",
      "11         0.841718        0.007902          0.905979         0.003456\n",
      "12         0.850543        0.015531          0.961402         0.001779\n",
      "13         0.865417        0.005247          0.966466         0.001701\n",
      "14         0.862560        0.006023          0.967648         0.001797\n",
      "15         0.864557        0.009319          0.968639         0.001796\n",
      "16         0.851085        0.013221          0.966240         0.001225\n",
      "17         0.860954        0.004406          0.971284         0.001987\n",
      "18         0.872224        0.009132          0.973767         0.001438\n",
      "19         0.872084        0.008899          0.974186         0.000325\n",
      "20         0.864537        0.010480          0.969673         0.001285\n",
      "21         0.872855        0.008067          0.972908         0.001392\n",
      "22         0.869074        0.008004          0.973960         0.002018\n",
      "23         0.871770        0.008420          0.973998         0.000818\n",
      "24         0.858277        0.008857          0.972583         0.001794\n",
      "25         0.862919        0.011621          0.975329         0.002477\n",
      "26         0.856382        0.014077          0.977560         0.001091\n",
      "27         0.867519        0.011928          0.978470         0.001428\n",
      "28         0.855721        0.020221          0.973128         0.002980\n",
      "29         0.874081        0.010064          0.976829         0.001010\n",
      "30         0.873830        0.014549          0.979023         0.001171\n",
      "31         0.870489        0.009849          0.979731         0.001392\n",
      "32         0.858158        0.015005          0.973911         0.001571\n",
      "33         0.863507        0.010694          0.978414         0.001731\n",
      "34         0.873979        0.010505          0.979416         0.000839\n",
      "35         0.879642        0.007074          0.979794         0.000700\n",
      "36         0.854714        0.013759          0.973029         0.000625\n",
      "37         0.857957        0.015256          0.975700         0.001197\n",
      "38         0.867939        0.011145          0.977246         0.002393\n",
      "39         0.861835        0.012747          0.979839         0.000950\n",
      "40         0.865075        0.009891          0.972788         0.001276\n",
      "41         0.871746        0.012045          0.977439         0.001202\n",
      "42         0.873667        0.006711          0.978833         0.000856\n",
      "43         0.869289        0.009459          0.979901         0.000872\n",
      "44         0.864063        0.007081          0.974128         0.000498\n",
      "45         0.868941        0.011394          0.978607         0.001341\n",
      "46         0.878076        0.005131          0.979882         0.001007\n",
      "47         0.873760        0.008929          0.979275         0.000480\n",
      "48         0.849730        0.010020          0.970973         0.001204\n",
      "49         0.864113        0.019163          0.976780         0.000808\n",
      "50         0.863161        0.012530          0.978049         0.001124\n",
      "51         0.865980        0.011686          0.978507         0.000547\n",
      "52         0.860279        0.009323          0.974178         0.002359\n",
      "53         0.866126        0.008925          0.976300         0.000821\n",
      "54         0.869498        0.013926          0.977967         0.000541\n",
      "55         0.871884        0.007550          0.978521         0.000911\n",
      "56         0.860825        0.011153          0.973001         0.001752\n",
      "57         0.869404        0.011226          0.975581         0.000953\n",
      "58         0.868760        0.007924          0.978252         0.001539\n",
      "59         0.872780        0.010908          0.979800         0.001040\n",
      "0    -0.047184\n",
      "1    -0.063297\n",
      "2    -0.058537\n",
      "3    -0.055054\n",
      "4    -0.060837\n",
      "5    -0.064607\n",
      "6    -0.058621\n",
      "7    -0.064843\n",
      "8    -0.068339\n",
      "9    -0.065846\n",
      "10   -0.062464\n",
      "11   -0.064261\n",
      "12   -0.110859\n",
      "13   -0.101049\n",
      "14   -0.105088\n",
      "15   -0.104082\n",
      "16   -0.115154\n",
      "17   -0.110330\n",
      "18   -0.101543\n",
      "19   -0.102102\n",
      "20   -0.105136\n",
      "21   -0.100053\n",
      "22   -0.104886\n",
      "23   -0.102229\n",
      "24   -0.114306\n",
      "25   -0.112410\n",
      "26   -0.121178\n",
      "27   -0.110951\n",
      "28   -0.117407\n",
      "29   -0.102748\n",
      "30   -0.105192\n",
      "31   -0.109242\n",
      "32   -0.115753\n",
      "33   -0.114907\n",
      "34   -0.105437\n",
      "35   -0.100151\n",
      "36   -0.118315\n",
      "37   -0.117743\n",
      "38   -0.109307\n",
      "39   -0.118004\n",
      "40   -0.107713\n",
      "41   -0.105693\n",
      "42   -0.105166\n",
      "43   -0.110613\n",
      "44   -0.110065\n",
      "45   -0.109666\n",
      "46   -0.101805\n",
      "47   -0.105515\n",
      "48   -0.121243\n",
      "49   -0.112667\n",
      "50   -0.114888\n",
      "51   -0.112527\n",
      "52   -0.113899\n",
      "53   -0.110174\n",
      "54   -0.108469\n",
      "55   -0.106637\n",
      "56   -0.112176\n",
      "57   -0.106177\n",
      "58   -0.109493\n",
      "59   -0.107019\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pipe_rf = [('preprocess', preprocess),\n",
    "             ('clf', RandomForestRegressor())]\n",
    "clf_rf = Pipeline(pipe_rf)\n",
    "params = {'clf__max_depth': range(5,26,5),\n",
    "         'clf__max_features': range(5,20,5),\n",
    "          'clf__n_estimators': [10,15,20,25]}\n",
    "grid = GridSearchCV(clf_rf, param_grid = params, cv = 5, return_train_score=True)\n",
    "best_rf = grid.fit(X_train,y_train).best_params_\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(results[['mean_test_score','std_test_score','mean_train_score','std_train_score']])\n",
    "print(results['mean_test_score']-results['mean_train_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_depth': 15, 'clf__max_features': 15, 'clf__n_estimators': 25}\n"
     ]
    }
   ],
   "source": [
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Scores\n",
      "[0.97897937 0.97760002 0.97982132]\n",
      "\n",
      "Test Scores\n",
      "[0.86779439 0.85884143 0.85965356]\n",
      "\n",
      "Train - Test Scores\n",
      "\n",
      "Appears to be some overfitting with the random forest regressor, however, we will still try to use in our stacked model\n"
     ]
    }
   ],
   "source": [
    "pipe_rf = [('scaler', preprocess),\n",
    "             ('clf', RandomForestRegressor(max_depth = best_rf['clf__max_depth'],\n",
    "                                           max_features = best_rf['clf__max_features'],\n",
    "                                           n_estimators = best_rf['clf__n_estimators']))]\n",
    "clf_elastic = Pipeline(pipe_rf)\n",
    "scores = cross_validate(clf_elastic, X_train, y_train, scoring = 'r2', return_train_score=True)\n",
    "print('Train Scores')\n",
    "print(scores['train_score'])\n",
    "print()\n",
    "print('Test Scores')\n",
    "print(scores['test_score'])\n",
    "print()\n",
    "print('Train - Test Scores')\n",
    "scores['train_score']-scores['test_score']\n",
    "\n",
    "print()\n",
    "print('Appears to be some overfitting with the random forest regressor, however, we will still try to use in our stacked model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall, appears our models are doing well with not too high of variance and low bias. Going to create a stacked model with\n",
    "# all four models\n",
    "\n",
    "# going to reshuffle the train-test splot\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([ 0.1,  1. , 10. ]), cv=5, fit_intercept=True,\n",
       "        gcv_mode=None, normalize=False, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up our first level predictions\n",
    "clf_lasso = LassoCV(cv=5)\n",
    "clf_elastic = ElasticNetCV(cv=5)\n",
    "clf_ridge = RidgeCV(cv=5)\n",
    "\n",
    "classifiers = [clf_lasso, clf_elastic, clf_ridge]\n",
    "predictions = []\n",
    "pred_cols = []\n",
    "\n",
    "for x in classifiers:\n",
    "    pipe_steps = [('preprocess', preprocess),\n",
    "                 ('clf', x)]\n",
    "    pipe = Pipeline(pipe_steps)\n",
    "    pred_name = 'pred_' + str(x)[0: str(x).find('(')]\n",
    "    pred_cols.append(pred_name)\n",
    "    pipe.fit(X_train,y_train)\n",
    "    predictions.append(transformer(pipe.predict(X_train)))\n",
    "    \n",
    "first_level_preds = pd.DataFrame({'pred_lasso': predictions[0],\n",
    "                                 'pred_elastic': predictions[1],\n",
    "                                 'pred_ridge': predictions[2]})\n",
    "\n",
    "X_train_2nd = pd.concat([X_train.reset_index(),first_level_preds], axis=1).set_index(keys='index')\n",
    "\n",
    "stacked = RidgeCV(cv = 5)\n",
    "stacked.fit(X_train_2nd,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8597938  0.91331485 0.91882004 0.86952702 0.94594065]\n",
      "0.032175214648617834\n",
      "\n",
      "0.11286465813262073\n",
      "0.927311374211476\n"
     ]
    }
   ],
   "source": [
    "# setting up our second level predictions\n",
    "\n",
    "predictions = []\n",
    "pred_cols = []\n",
    "\n",
    "for x in classifiers:\n",
    "    pipe_steps = [('preprocess', preprocess),\n",
    "                 ('clf', x)]\n",
    "    pipe = Pipeline(pipe_steps)\n",
    "    pred_name = 'pred_' + str(x)[0: str(x).find('(')]\n",
    "    pred_cols.append(pred_name)\n",
    "    predictions.append(transformer(pipe.predict(X_test)))\n",
    "    \n",
    "test_level_preds = pd.DataFrame({'pred_lasso': predictions[0],\n",
    "                                 'pred_elastic': predictions[1],\n",
    "                                 'pred_ridge': predictions[2]})\n",
    "\n",
    "X_test_1st = pd.concat([X_test.reset_index(),test_level_preds], axis=1).set_index(keys='index')\n",
    "\n",
    "results = cross_val_score(stacked, X_train, y_train, scoring = 'r2', cv = 5)\n",
    "print(results)\n",
    "print(np.std(results))\n",
    "print()\n",
    "\n",
    "stacked_preds = stacked.predict(X_test_1st)\n",
    "print(np.sqrt(mean_squared_log_error(transformer(y_test),transformer(stacked_preds))))\n",
    "print(r2_score(transformer(y_test),transformer(stacked_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There we go. As we can see, we were able to improve our model slightly by stacking our regressors. In the future, we could\n",
    "# try to add some more regressors with high performance that were slightly less correlated with our other models to try improve\n",
    "# the stacking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for if I decide to try with dropped variables, although no longer necessary \\n\\n\\ndrop_list = []\\ndrop_list.append('Condition1')\\ndrop_list.append('HasGarage')\\ndrop_list.append('LotConfig')\\ndrop_list.append('RoofStyle')\\ndrop_list.append('BsmtFinType1')\\ndrop_list.append('Functional')\\ndrop_list.append('LandContour')\\ndrop_list.append('GarageType')\\n\\nfor x in list(df.select_dtypes(include = ['int64', 'float64']).columns):\\n    if '*' in x:\\n        drop_list.append(x)\\n        \\ndf = pd.read_csv('ml_ready.csv').drop(['Unnamed: 0', 'Unnamed: 0.1', 'GrLivAreaNegSq', 'TotalBsmtSFNegSq'], axis=1)\\ndf = df.drop(drop_list, axis=1)\\ndf['YearBuiltSq'] = df['YearBuilt']*df['YearBuilt']\\n\\ncategorical = []\\nfor x in list(df.columns):\\n    if 'Non' in x:\\n        categorical.append(x)\\n    elif 'Has' in x:\\n        categorical.append(x)\\n    elif 'SubClass' in x:\\n        categorical.append(x)\\n    elif 'SaleConditionNor' in x:\\n        categorical.append(x)\\n    elif 'LandSlopeGtl' in x:\\n        categorical.append(x)\\n    elif 'LotShapeReg' in x:\\n        categorical.append(x)\\n    elif 'HeatingGas' in x:\\n        categorical.append(x)      \\n    elif 'CentralAir' in x:\\n        categorical.append(x)\\n    elif 'MoSold' in x:\\n        categorical.append(x)\\n    elif 'YrSold' in x:\\n        categorical.append(x)\\n    \\nfor x in list(df.select_dtypes(include='object').columns):\\n    categorical.append(x)\\ncategorical = list(set(categorical))\\n\\n# merging converted dfs\\n\\ndf[categorical] = df[categorical].astype('object')\\ndf_num = df.drop(categorical, axis = 1)\\ndf = pd.concat([df[categorical],df_num], axis=1)\\n\\n# creating the X and y variables\\n\\nX = df.drop(['LogSale','SalePrice'],axis=1)\\nX = pd.get_dummies(X,drop_first=True)\\ny = df['LogSale']\\n\\nnumerical = list(X.select_dtypes(include = ['int64', 'float64']).columns)\\nskewed = X[numerical].skew(axis=0).reset_index()\\nskewed_vars = list(skewed[abs(skewed[0])>2]['index'])\\n\\n# creating test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33)\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" for if I decide to try with dropped variables, although no longer necessary \n",
    "\n",
    "\n",
    "drop_list = []\n",
    "drop_list.append('Condition1')\n",
    "drop_list.append('HasGarage')\n",
    "drop_list.append('LotConfig')\n",
    "drop_list.append('RoofStyle')\n",
    "drop_list.append('BsmtFinType1')\n",
    "drop_list.append('Functional')\n",
    "drop_list.append('LandContour')\n",
    "drop_list.append('GarageType')\n",
    "\n",
    "for x in list(df.select_dtypes(include = ['int64', 'float64']).columns):\n",
    "    if '*' in x:\n",
    "        drop_list.append(x)\n",
    "        \n",
    "df = pd.read_csv('ml_ready.csv').drop(['Unnamed: 0', 'Unnamed: 0.1', 'GrLivAreaNegSq', 'TotalBsmtSFNegSq'], axis=1)\n",
    "df = df.drop(drop_list, axis=1)\n",
    "df['YearBuiltSq'] = df['YearBuilt']*df['YearBuilt']\n",
    "\n",
    "categorical = []\n",
    "for x in list(df.columns):\n",
    "    if 'Non' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'Has' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'SubClass' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'SaleConditionNor' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'LandSlopeGtl' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'LotShapeReg' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'HeatingGas' in x:\n",
    "        categorical.append(x)      \n",
    "    elif 'CentralAir' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'MoSold' in x:\n",
    "        categorical.append(x)\n",
    "    elif 'YrSold' in x:\n",
    "        categorical.append(x)\n",
    "    \n",
    "for x in list(df.select_dtypes(include='object').columns):\n",
    "    categorical.append(x)\n",
    "categorical = list(set(categorical))\n",
    "\n",
    "# merging converted dfs\n",
    "\n",
    "df[categorical] = df[categorical].astype('object')\n",
    "df_num = df.drop(categorical, axis = 1)\n",
    "df = pd.concat([df[categorical],df_num], axis=1)\n",
    "\n",
    "# creating the X and y variables\n",
    "\n",
    "X = df.drop(['LogSale','SalePrice'],axis=1)\n",
    "X = pd.get_dummies(X,drop_first=True)\n",
    "y = df['LogSale']\n",
    "\n",
    "numerical = list(X.select_dtypes(include = ['int64', 'float64']).columns)\n",
    "skewed = X[numerical].skew(axis=0).reset_index()\n",
    "skewed_vars = list(skewed[abs(skewed[0])>2]['index'])\n",
    "\n",
    "# creating test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
